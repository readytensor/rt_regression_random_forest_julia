{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES.\n",
    "using Suppressor\n",
    "@suppress begin\n",
    "    using DataFrames, CSV, Random, Statistics, Serialization, LazyJSON, StatsBase, DecisionTree\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES \n",
    "ROOT_DIR = dirname(pwd())\n",
    "MODEL_INPUTS_OUTPUTS = joinpath(ROOT_DIR, \"model_inputs_outputs\")\n",
    "INPUT_DIR = joinpath(MODEL_INPUTS_OUTPUTS, \"inputs\")\n",
    "INPUT_SCHEMA_DIR = joinpath(INPUT_DIR, \"schema\")\n",
    "DATA_DIR = joinpath(INPUT_DIR, \"data\")\n",
    "OUTPUT_DIR = joinpath(MODEL_INPUTS_OUTPUTS, \"outputs\")\n",
    "TRAIN_DIR = joinpath(DATA_DIR, \"training\")\n",
    "TEST_DIR = joinpath(DATA_DIR, \"testing\")\n",
    "MODEL_PATH = joinpath(MODEL_INPUTS_OUTPUTS, \"model\")\n",
    "MODEL_ARTIFACTS_PATH = joinpath(MODEL_PATH, \"artifacts\")\n",
    "OHE_ENCODER_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"ohe.jld2\")\n",
    "PREDICTOR_DIR_PATH = joinpath(MODEL_ARTIFACTS_PATH, \"predictor\")\n",
    "PREDICTOR_FILE_PATH = joinpath(PREDICTOR_DIR_PATH, \"predictor.ser\")\n",
    "IMPUTATION_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"imputation.ser\")\n",
    "TOP_CATEGORIES = joinpath(MODEL_ARTIFACTS_PATH, \"top_categories.ser\")\n",
    "PREDICTIONS_DIR = joinpath(OUTPUT_DIR, \"predictions\")\n",
    "PREDICTIONS_FILE = joinpath(PREDICTIONS_DIR, \"predictions.csv\")\n",
    "\n",
    "\n",
    "if !isdir(MODEL_ARTIFACTS_PATH)\n",
    "    mkdir(MODEL_ARTIFACTS_PATH)\n",
    "end\n",
    "if !isdir(PREDICTOR_DIR_PATH)\n",
    "    mkdir(PREDICTOR_DIR_PATH)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"target\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a schema from a JSON file and extracting features\n",
    "file_name = first(filter(x -> endswith(x, \"json\"), readdir(INPUT_SCHEMA_DIR)))\n",
    "schema_path = joinpath(INPUT_SCHEMA_DIR, file_name)\n",
    "schema_string = read(schema_path, String)  # Read file content as a string\n",
    "schema = LazyJSON.parse(schema_string)\n",
    "features = schema[\"features\"]\n",
    "\n",
    "# Identifying numeric, categorical, and nullable features\n",
    "numeric_features = String[]\n",
    "categorical_features = String[]\n",
    "nullable_features = String[]\n",
    "\n",
    "for f in features\n",
    "    if f[\"dataType\"] == \"CATEGORICAL\"\n",
    "        push!(categorical_features, f[\"name\"])\n",
    "    else\n",
    "        push!(numeric_features, f[\"name\"])\n",
    "    end\n",
    "    if f[\"nullable\"]\n",
    "        push!(nullable_features, f[\"name\"])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Extracting ID and target features\n",
    "id_feature = schema[\"id\"][\"name\"]\n",
    "target_feature = schema[\"target\"][\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>40 rows × 3 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>id</th><th>number</th><th>color</th></tr><tr><th></th><th title=\"String7\">String7</th><th title=\"Union{Missing, Float64}\">Float64?</th><th title=\"Union{Missing, String7}\">String7?</th></tr></thead><tbody><tr><th>1</th><td>RJD27C</td><td>1.8403</td><td>Blue</td></tr><tr><th>2</th><td>PFQ2ZK</td><td>7.2176</td><td>Green</td></tr><tr><th>3</th><td>Y5K92G</td><td>5.4254</td><td><em>missing</em></td></tr><tr><th>4</th><td>NPVTQJ</td><td>4.7513</td><td>Blue</td></tr><tr><th>5</th><td>3YMG1J</td><td>3.2551</td><td>Red</td></tr><tr><th>6</th><td>CLM1X9</td><td>1.6502</td><td>Blue</td></tr><tr><th>7</th><td>CJPH64</td><td>0.3709</td><td>Blue</td></tr><tr><th>8</th><td>BROW8B</td><td><em>missing</em></td><td>Green</td></tr><tr><th>9</th><td>8ANJT0</td><td>3.4635</td><td>Blue</td></tr><tr><th>10</th><td>RVS8XI</td><td>7.446</td><td>Green</td></tr><tr><th>11</th><td>JOY8VT</td><td>6.1574</td><td>Green</td></tr><tr><th>12</th><td>5TTDYK</td><td>1.0462</td><td>Blue</td></tr><tr><th>13</th><td>LHR3TK</td><td>5.1219</td><td>Red</td></tr><tr><th>14</th><td>Y5P7R7</td><td>2.9188</td><td>Blue</td></tr><tr><th>15</th><td>Q51Z2W</td><td>2.0595</td><td><em>missing</em></td></tr><tr><th>16</th><td>INRPE5</td><td>0.9011</td><td>Green</td></tr><tr><th>17</th><td>KYOG64</td><td>8.2612</td><td>Red</td></tr><tr><th>18</th><td>U6KO0P</td><td>1.6911</td><td>Green</td></tr><tr><th>19</th><td>SM1D1I</td><td>1.7481</td><td>Blue</td></tr><tr><th>20</th><td>CJRUQ7</td><td>7.1583</td><td>Blue</td></tr><tr><th>21</th><td>8CDU3T</td><td>3.8511</td><td>Blue</td></tr><tr><th>22</th><td>O1US28</td><td>3.3868</td><td>Blue</td></tr><tr><th>23</th><td>OA8CVU</td><td>6.1253</td><td>Red</td></tr><tr><th>24</th><td>LNWBSJ</td><td><em>missing</em></td><td>Green</td></tr><tr><th>25</th><td>9KDY33</td><td>3.0546</td><td><em>missing</em></td></tr><tr><th>26</th><td>PJ3EP5</td><td>0.9344</td><td>Green</td></tr><tr><th>27</th><td>NKDFXX</td><td><em>missing</em></td><td>Blue</td></tr><tr><th>28</th><td>M9USMA</td><td>5.1233</td><td><em>missing</em></td></tr><tr><th>29</th><td>GJ7ORO</td><td>4.567</td><td>Red</td></tr><tr><th>30</th><td>LIZ88M</td><td>0.254</td><td>Red</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& id & number & color\\\\\n",
       "\t\\hline\n",
       "\t& String7 & Float64? & String7?\\\\\n",
       "\t\\hline\n",
       "\t1 & RJD27C & 1.8403 & Blue \\\\\n",
       "\t2 & PFQ2ZK & 7.2176 & Green \\\\\n",
       "\t3 & Y5K92G & 5.4254 & \\emph{missing} \\\\\n",
       "\t4 & NPVTQJ & 4.7513 & Blue \\\\\n",
       "\t5 & 3YMG1J & 3.2551 & Red \\\\\n",
       "\t6 & CLM1X9 & 1.6502 & Blue \\\\\n",
       "\t7 & CJPH64 & 0.3709 & Blue \\\\\n",
       "\t8 & BROW8B & \\emph{missing} & Green \\\\\n",
       "\t9 & 8ANJT0 & 3.4635 & Blue \\\\\n",
       "\t10 & RVS8XI & 7.446 & Green \\\\\n",
       "\t11 & JOY8VT & 6.1574 & Green \\\\\n",
       "\t12 & 5TTDYK & 1.0462 & Blue \\\\\n",
       "\t13 & LHR3TK & 5.1219 & Red \\\\\n",
       "\t14 & Y5P7R7 & 2.9188 & Blue \\\\\n",
       "\t15 & Q51Z2W & 2.0595 & \\emph{missing} \\\\\n",
       "\t16 & INRPE5 & 0.9011 & Green \\\\\n",
       "\t17 & KYOG64 & 8.2612 & Red \\\\\n",
       "\t18 & U6KO0P & 1.6911 & Green \\\\\n",
       "\t19 & SM1D1I & 1.7481 & Blue \\\\\n",
       "\t20 & CJRUQ7 & 7.1583 & Blue \\\\\n",
       "\t21 & 8CDU3T & 3.8511 & Blue \\\\\n",
       "\t22 & O1US28 & 3.3868 & Blue \\\\\n",
       "\t23 & OA8CVU & 6.1253 & Red \\\\\n",
       "\t24 & LNWBSJ & \\emph{missing} & Green \\\\\n",
       "\t25 & 9KDY33 & 3.0546 & \\emph{missing} \\\\\n",
       "\t26 & PJ3EP5 & 0.9344 & Green \\\\\n",
       "\t27 & NKDFXX & \\emph{missing} & Blue \\\\\n",
       "\t28 & M9USMA & 5.1233 & \\emph{missing} \\\\\n",
       "\t29 & GJ7ORO & 4.567 & Red \\\\\n",
       "\t30 & LIZ88M & 0.254 & Red \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m40×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m id      \u001b[0m\u001b[1m number       \u001b[0m\u001b[1m color    \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m String7 \u001b[0m\u001b[90m Float64?     \u001b[0m\u001b[90m String7? \u001b[0m\n",
       "─────┼─────────────────────────────────\n",
       "   1 │ RJD27C         1.8403  Blue\n",
       "   2 │ PFQ2ZK         7.2176  Green\n",
       "   3 │ Y5K92G         5.4254 \u001b[90m missing  \u001b[0m\n",
       "   4 │ NPVTQJ         4.7513  Blue\n",
       "   5 │ 3YMG1J         3.2551  Red\n",
       "   6 │ CLM1X9         1.6502  Blue\n",
       "   7 │ CJPH64         0.3709  Blue\n",
       "   8 │ BROW8B  \u001b[90m missing      \u001b[0m Green\n",
       "   9 │ 8ANJT0         3.4635  Blue\n",
       "  10 │ RVS8XI         7.446   Green\n",
       "  11 │ JOY8VT         6.1574  Green\n",
       "  ⋮  │    ⋮          ⋮           ⋮\n",
       "  31 │ XEIQQL         7.7383  Blue\n",
       "  32 │ DCXZUK         8.5685  Green\n",
       "  33 │ W6S0G2         7.8106  Blue\n",
       "  34 │ V6SYQY         4.5311  Green\n",
       "  35 │ 9KICO0         0.3756  Blue\n",
       "  36 │ DWFJG3         3.7334 \u001b[90m missing  \u001b[0m\n",
       "  37 │ 4BHS1P         6.1777  Red\n",
       "  38 │ PDGERP         1.0688 \u001b[90m missing  \u001b[0m\n",
       "  39 │ 0D2XAN         8.0007  Blue\n",
       "  40 │ BBKRZM         1.662  \u001b[90m missing  \u001b[0m\n",
       "\u001b[36m                        19 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = filter(x -> occursin(\".csv\", x), readdir(TEST_DIR))[1]\n",
    "file_path = joinpath(TEST_DIR, file_name)\n",
    "df = DataFrame(CSV.File(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "Note that when we work with testing data, we have to impute using the same values learned during training. This is to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>40 rows × 2 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>number</th><th>color</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"String7\">String7</th></tr></thead><tbody><tr><th>1</th><td>1.8403</td><td>Blue</td></tr><tr><th>2</th><td>7.2176</td><td>Green</td></tr><tr><th>3</th><td>5.4254</td><td>Blue</td></tr><tr><th>4</th><td>4.7513</td><td>Blue</td></tr><tr><th>5</th><td>3.2551</td><td>Red</td></tr><tr><th>6</th><td>1.6502</td><td>Blue</td></tr><tr><th>7</th><td>0.3709</td><td>Blue</td></tr><tr><th>8</th><td>5.05815</td><td>Green</td></tr><tr><th>9</th><td>3.4635</td><td>Blue</td></tr><tr><th>10</th><td>7.446</td><td>Green</td></tr><tr><th>11</th><td>6.1574</td><td>Green</td></tr><tr><th>12</th><td>1.0462</td><td>Blue</td></tr><tr><th>13</th><td>5.1219</td><td>Red</td></tr><tr><th>14</th><td>2.9188</td><td>Blue</td></tr><tr><th>15</th><td>2.0595</td><td>Blue</td></tr><tr><th>16</th><td>0.9011</td><td>Green</td></tr><tr><th>17</th><td>8.2612</td><td>Red</td></tr><tr><th>18</th><td>1.6911</td><td>Green</td></tr><tr><th>19</th><td>1.7481</td><td>Blue</td></tr><tr><th>20</th><td>7.1583</td><td>Blue</td></tr><tr><th>21</th><td>3.8511</td><td>Blue</td></tr><tr><th>22</th><td>3.3868</td><td>Blue</td></tr><tr><th>23</th><td>6.1253</td><td>Red</td></tr><tr><th>24</th><td>5.05815</td><td>Green</td></tr><tr><th>25</th><td>3.0546</td><td>Blue</td></tr><tr><th>26</th><td>0.9344</td><td>Green</td></tr><tr><th>27</th><td>5.05815</td><td>Blue</td></tr><tr><th>28</th><td>5.1233</td><td>Blue</td></tr><tr><th>29</th><td>4.567</td><td>Red</td></tr><tr><th>30</th><td>0.254</td><td>Red</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& number & color\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & String7\\\\\n",
       "\t\\hline\n",
       "\t1 & 1.8403 & Blue \\\\\n",
       "\t2 & 7.2176 & Green \\\\\n",
       "\t3 & 5.4254 & Blue \\\\\n",
       "\t4 & 4.7513 & Blue \\\\\n",
       "\t5 & 3.2551 & Red \\\\\n",
       "\t6 & 1.6502 & Blue \\\\\n",
       "\t7 & 0.3709 & Blue \\\\\n",
       "\t8 & 5.05815 & Green \\\\\n",
       "\t9 & 3.4635 & Blue \\\\\n",
       "\t10 & 7.446 & Green \\\\\n",
       "\t11 & 6.1574 & Green \\\\\n",
       "\t12 & 1.0462 & Blue \\\\\n",
       "\t13 & 5.1219 & Red \\\\\n",
       "\t14 & 2.9188 & Blue \\\\\n",
       "\t15 & 2.0595 & Blue \\\\\n",
       "\t16 & 0.9011 & Green \\\\\n",
       "\t17 & 8.2612 & Red \\\\\n",
       "\t18 & 1.6911 & Green \\\\\n",
       "\t19 & 1.7481 & Blue \\\\\n",
       "\t20 & 7.1583 & Blue \\\\\n",
       "\t21 & 3.8511 & Blue \\\\\n",
       "\t22 & 3.3868 & Blue \\\\\n",
       "\t23 & 6.1253 & Red \\\\\n",
       "\t24 & 5.05815 & Green \\\\\n",
       "\t25 & 3.0546 & Blue \\\\\n",
       "\t26 & 0.9344 & Green \\\\\n",
       "\t27 & 5.05815 & Blue \\\\\n",
       "\t28 & 5.1233 & Blue \\\\\n",
       "\t29 & 4.567 & Red \\\\\n",
       "\t30 & 0.254 & Red \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m40×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m number  \u001b[0m\u001b[1m color   \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m String7 \u001b[0m\n",
       "─────┼──────────────────\n",
       "   1 │ 1.8403   Blue\n",
       "   2 │ 7.2176   Green\n",
       "   3 │ 5.4254   Blue\n",
       "   4 │ 4.7513   Blue\n",
       "   5 │ 3.2551   Red\n",
       "   6 │ 1.6502   Blue\n",
       "   7 │ 0.3709   Blue\n",
       "   8 │ 5.05815  Green\n",
       "   9 │ 3.4635   Blue\n",
       "  10 │ 7.446    Green\n",
       "  11 │ 6.1574   Green\n",
       "  ⋮  │    ⋮        ⋮\n",
       "  31 │ 7.7383   Blue\n",
       "  32 │ 8.5685   Green\n",
       "  33 │ 7.8106   Blue\n",
       "  34 │ 4.5311   Green\n",
       "  35 │ 0.3756   Blue\n",
       "  36 │ 3.7334   Blue\n",
       "  37 │ 6.1777   Red\n",
       "  38 │ 1.0688   Blue\n",
       "  39 │ 8.0007   Blue\n",
       "  40 │ 1.662    Blue\n",
       "\u001b[36m         19 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputation_values = open(deserialize, IMPUTATION_FILE)\n",
    "for column in nullable_features\n",
    "    df[!, Symbol(column)] .= coalesce.(df[!, Symbol(column)], get(imputation_values, string(column), missing))\n",
    "end\n",
    "\n",
    "# Saving the id column in a different variable\n",
    "ids = df[!, Symbol(id_feature)]\n",
    "\n",
    "# Dropping the id and target from the DataFrame\n",
    "select!(df, Not([Symbol(id_feature)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "We encode the data using the same encoder that we saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_top_categories = open(deserialize, TOP_CATEGORIES)\n",
    "\n",
    "# Function to one-hot encode only the top 3 categories\n",
    "function one_hot_top_categories!(df, top_categories)\n",
    "    for (feature, top_cats) in top_categories\n",
    "        if length(top_cats) == 2  # Handle the binary case\n",
    "            # Assuming the first category in top_cats is treated as 'true'\n",
    "            new_col_name = \"$(feature)_binary\"\n",
    "            df[!, new_col_name] = df[!, feature] .== top_cats[1]\n",
    "        else  # Handle the general case\n",
    "            for cat in top_cats\n",
    "                new_col_name = \"$(feature)_$(cat)\"\n",
    "                df[!, new_col_name] = df[!, feature] .== cat\n",
    "            end\n",
    "        end\n",
    "        select!(df, Not(Symbol(feature)))  # Drop the original feature column\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "one_hot_top_categories!(df, loaded_top_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions & Creating Predictions DataFrame\n",
    "Using the model saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/moo/Desktop/Upwork/rt-ML/Regression/Julia/Julia-Linear-Regression-Random-Forest/model_inputs_outputs/outputs/predictions/predictions.csv\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = open(deserialize, PREDICTOR_FILE_PATH)\n",
    "predictions = apply_forest(model, Matrix(df))\n",
    "\n",
    "id_column_name = id_feature\n",
    "prediction_column_name = \"prediction\"\n",
    "# Create a DataFrame\n",
    "df = DataFrame()\n",
    "df[!, Symbol(id_column_name)] = ids\n",
    "df[!, Symbol(prediction_column_name)] = predictions\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "CSV.write(PREDICTIONS_FILE, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
